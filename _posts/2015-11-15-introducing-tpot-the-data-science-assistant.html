---
layout: post
title: Introducing TPOT, the Data Science Assistant
date: 2015-11-15 17:55:49.000000000 -08:00
type: post
post-image: /assets/2015/11/rso-facebook-face-detection.png
parent_id: '0'
published: true
password: ''
status: publish
categories:
- machine learning
- python
- research
tags:
- automation
- data science
- evolutionary computing
- machine learning
- pipeline
- python
meta:
  _responsive_layout: default
  _edit_last: '2'
  _oembed_time_8de556776ac15bce4d6b537c31b15dbd: '1472907254'
  _oembed_8de556776ac15bce4d6b537c31b15dbd: <blockquote class="twitter-tweet" data-width="550"><p
    lang="und" dir="ltr"><a href="https://twitter.com/randal_olson">@randal_olson</a>
    <a href="https://t.co/ds5iTbA2oF">pic.twitter.com/ds5iTbA2oF</a></p>&mdash; Justin
    Kiggins (@neuromusic) <a href="https://twitter.com/neuromusic/status/665190991792463872">November
    13, 2015</a></blockquote><script async src="//platform.twitter.com/widgets.js"
    charset="utf-8"></script>
  _aioseop_title: Introducing TPOT, the Data Science Assistant
  dsq_thread_id: '4321984190'
  _aioseop_keywords: machine learning, data science, pipeline, automation, python,
    evolutionary computing, artificial intelligence, genetic programming, scikit-learn
author:
  login: rhiever
  email: rhiever@gmail.com
  display_name: Randy Olson
  first_name: Randy
  last_name: Olson
permalink: "/2015/11/15/introducing-tpot-the-data-science-assistant/"
---
<p>Some of you might have been wondering what the heck I've been up to for the past few months. I haven't been posting much on my blog lately, and I haven't been working on important problems like <a href="http://www.randalolson.com/2015/02/03/heres-waldo-computing-the-optimal-search-strategy-for-finding-waldo/">solving <em>Where's Waldo?</em></a> and <a href="http://www.randalolson.com/2015/03/08/computing-the-optimal-road-trip-across-the-u-s/">optimizing road trips around the world</a>. (I promise: I'll get back to fun posts like that soon!) Instead, I've been working on something far geekier, and I'm excited to finally have something to show for it.</p>
<p>Over the summer, I started a new postdoctoral research position funded by the <a href="http://www.nih.gov/">NIH</a> at the University of Pennsylvania <a href="http://www.epistasis.org/">Computational Genetics Lab</a>. During my first month there, I started looking for big problems in the field of data science to take on. Science (especially computer science) is often too incremental, and if I was going to stay in academia, I wanted to tackle a big problem. It was around that time that I started thinking about the <a href="https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb">process of machine learning</a> and how we could let machines solve problems themselves rather than needing input from humans.</p>
<p>You see, machine learning is transforming the world as we know it. Google search engines were massively improved by machine learning, as were Gmail's spam filters. Voice assistants like Siri -- <a href="http://www.businessinsider.com/best-siri-responses-2015-6">as silly as they can be</a> -- use machine learning to translate your voice into something the computer can understand. Stock market investors make millions every day using machine learning to predict when to buy and sell. And the list goes on and on...</p>
<p>[caption id="attachment_7681" align="aligncenter" width="600" caption="Ever wonder how Facebook always knows who you are in your photos? They use machine learning."]<a href="http://www.randalolson.com/wp-content/uploads/rso-facebook-face-detection.png"><img src="{{ site.baseurl }}/assets/2015/11/rso-facebook-face-detection.png" alt="Wonder how Facebook always knows who you are in your photos? They use machine learning." width="600" class="size-full wp-image-7681" /></a>[/caption]</p>
<p>The problem with machine learning is that building an effective model can require a ton of human input. Humans have to figure out the right way to transform the data before feeding it to the machine learning model. Then they have to pick the right machine learning model that will learn from the data best, and then there's a whole bunch of model parameters to tweak that can make the difference between a dud and a Nostradamus-like model. Building these pipelines -- i.e., sequences of steps that turn the raw data into a predictive model -- can easily take weeks of tinkering depending on the difficulty of the problem. This is obviously a huge issue when machine learning is supposed to allow machines to learn on their own.</p>
<p>[caption align="aligncenter" width="600" caption="An example machine learning pipeline, and what parts of the pipeline TPOT automates."]<a href="https://raw.githubusercontent.com/rhiever/tpot/master/images/tpot-ml-pipeline.png"><img src="{{ site.baseurl }}/assets/2015/11/tpot-ml-pipeline.png" alt="An example machine learning pipeline" width="600" class="size-full" /></a>[/caption]</p>
<p>Thus, the <a href="https://github.com/rhiever/tpot">Tree-based Pipeline Optimization Tool (TPOT)</a> was born. TPOT is a Python tool that automatically creates and optimizes machine learning pipelines using <a href="https://en.wikipedia.org/wiki/Genetic_programming">genetic programming</a>. Think of TPOT as your "Data Science Assistant": TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines, then recommending the pipelines that work best for your data.</p>
<p>[caption align="aligncenter" width="600" caption="An example TPOT pipeline with two copies of the data set entering the pipeline."]<a href="https://raw.githubusercontent.com/rhiever/tpot/master/images/tpot-pipeline-example.png"><img src="{{ site.baseurl }}/assets/2015/11/tpot-pipeline-example.png" alt="An example tree-based pipeline with two copies of the data set entering the pipeline." width="600" class="size-full" /></a>[/caption]</p>
<p>Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there. As an added bonus, TPOT is built on top of <a href="http://scikit-learn.org/">scikit-learn</a>, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.</p>
<p><strong>TPOT is still under active development</strong> and in its early stages, but it's worked very well on the classification problems I've applied it to so far.</p>
<p><a href="https://github.com/rhiever/tpot"><strong>Check out the TPOT GitHub repository</strong></a> to see the latest goings on. I'll be working on TPOT and pushing the boundaries of machine learning pipeline optimization for the majority of my postdoc.</p>
<h3>An example using TPOT</h3>
<p>I wanted to make TPOT versatile, so it can be used on the command line or via Python scripts. You can look up the <a href="https://github.com/rhiever/tpot#usage">detailed usage instructions</a> on the GitHub repository if you're interested.</p>
<p>For this post, I've provided a basic example of how you can use TPOT to build a pipeline that classifies hand-written digits in the classic <a href="http://yann.lecun.com/exdb/mnist/">MNIST data set</a>.</p>
<pre name="code" class="python:nogutter">
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.cross_validation import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
</pre>
<p>After 10 or so minutes, TPOT will discover a pipeline that achieves roughly 98% accuracy. In this case, TPOT will probably discover that a <a href="https://en.wikipedia.org/wiki/Random_forest">random forest classifier</a> and k-nearest-neighbor classifier does very well on MNIST with only a little bit of tuning. If you give TPOT even more time by setting the "generations" parameter to a higher number, it may find even better pipelines.</p>
<h3>"TPOT sounds cool! How can I get involved?"</h3>
<p>TPOT is an open source project, and I'm happy to have you join our efforts to build the best tool possible. If you want to contribute some code, <a href="https://github.com/rhiever/tpot/issues/">check the existing issues</a> for bugs or enhancements to work on. If you have an idea for an extension to TPOT, <a href="https://github.com/rhiever/tpot/issues/new">please file a new issue</a> so we can discuss it.</p>
<h3>tl;dr in image format</h3>
<p><a href="https://twitter.com/neuromusic">Justin Kiggins</a> had a great summary of TPOT when I first tweeted about it:</p>
<p>https://twitter.com/neuromusic/status/665190991792463872</p>
<p>Anyway, that's what I've been up to lately. I'm looking forward to presenting TPOT at several research conferences in the coming months, and I'd really like to see what the machine learning community thinks about pipeline automation. In the meantime, <a href="http://EpistasisLab.github.io/tpot/installing/">give TPOT a try</a> and let me know what you think.</p>
